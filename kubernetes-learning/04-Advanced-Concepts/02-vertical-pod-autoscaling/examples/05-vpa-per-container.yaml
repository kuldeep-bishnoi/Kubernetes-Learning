---
# VPA with Per-Container Configuration Example
apiVersion: v1
kind: Namespace
metadata:
  name: vpa-per-container-demo
---
# ConfigMap for the main application
apiVersion: v1
kind: ConfigMap
metadata:
  name: main-app-config
  namespace: vpa-per-container-demo
data:
  index.html: |
    <!DOCTYPE html>
    <html>
    <head>
      <title>Multi-Container VPA Demo</title>
      <style>
        body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
        h1 { color: #333; }
        .container { display: flex; flex-wrap: wrap; }
        .box { flex: 1; min-width: 250px; margin: 10px; padding: 20px; border: 1px solid #ddd; border-radius: 4px; }
        .btn { background: #4CAF50; color: white; border: none; padding: 10px 15px; border-radius: 4px; cursor: pointer; }
        .btn:hover { background: #45a049; }
      </style>
    </head>
    <body>
      <h1>Multi-Container VPA Demo</h1>
      <div class="container">
        <div class="box">
          <h2>Main Application</h2>
          <p>This container serves web content and handles user requests.</p>
          <p><a href="/metrics">View Metrics</a></p>
        </div>
        <div class="box">
          <h2>Cache Container</h2>
          <p>A Redis cache for fast data access with configurable memory usage.</p>
          <p><button class="btn" onclick="fetch('/api/cache/load')">Load Cache Data</button></p>
          <p><button class="btn" onclick="fetch('/api/cache/clear')">Clear Cache</button></p>
        </div>
        <div class="box">
          <h2>Logger Container</h2>
          <p>Collects and processes application logs with variable CPU usage.</p>
          <p><button class="btn" onclick="fetch('/api/logs/generate?count=1000')">Generate Logs</button></p>
          <p><button class="btn" onclick="fetch('/api/logs/analyze')">Analyze Logs</button></p>
        </div>
      </div>
    </body>
    </html>
  nginx.conf: |
    server {
      listen 80;
      server_name localhost;
      
      location / {
        root /usr/share/nginx/html;
        index index.html;
      }
      
      location /api/cache/ {
        proxy_pass http://localhost:8081/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
      }
      
      location /api/logs/ {
        proxy_pass http://localhost:8082/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
      }
      
      location /metrics {
        stub_status on;
        access_log off;
      }
    }
---
# ConfigMap for the cache service
apiVersion: v1
kind: ConfigMap
metadata:
  name: cache-service-config
  namespace: vpa-per-container-demo
data:
  cache-service.py: |
    #!/usr/bin/env python3
    from flask import Flask, jsonify, request
    import os
    import time
    import random
    import string
    import threading
    
    app = Flask(__name__)
    
    # Simulated cache with memory usage control
    cache_data = {}
    cache_lock = threading.Lock()
    
    # Generate random string of specified size (in KB)
    def generate_data(size_kb):
        # Each character is ~1 byte, so 1KB is ~1000 characters
        return ''.join(random.choices(string.ascii_letters + string.digits, k=size_kb * 1000))
    
    @app.route('/')
    def home():
        return jsonify({"service": "cache", "status": "running"})
    
    @app.route('/load')
    def load_cache():
        """Load data into cache with specified memory usage"""
        size_mb = int(request.args.get('size_mb', 50))  # Default 50MB
        entries = int(request.args.get('entries', 1000))  # Default 1000 entries
        
        with cache_lock:
            # Clear existing cache
            cache_data.clear()
            
            # Calculate size per entry
            size_kb_per_entry = (size_mb * 1024) // entries
            
            # Generate and store random data
            for i in range(entries):
                key = f"key_{i}"
                cache_data[key] = generate_data(size_kb_per_entry)
        
        return jsonify({
            "status": "success",
            "loaded_entries": entries,
            "approx_size_mb": size_mb
        })
    
    @app.route('/clear')
    def clear_cache():
        """Clear the cache"""
        with cache_lock:
            cache_data.clear()
        
        return jsonify({"status": "success", "message": "Cache cleared"})
    
    @app.route('/status')
    def cache_status():
        """Return cache status"""
        with cache_lock:
            entry_count = len(cache_data)
            # Rough estimate of memory usage
            if entry_count > 0:
                sample_key = next(iter(cache_data))
                sample_size = len(cache_data[sample_key])
                estimated_size_mb = (entry_count * sample_size) / (1024 * 1024)
            else:
                estimated_size_mb = 0
        
        return jsonify({
            "entries": entry_count,
            "estimated_size_mb": estimated_size_mb
        })
    
    if __name__ == '__main__':
        app.run(host='0.0.0.0', port=8081)
---
# ConfigMap for the log service
apiVersion: v1
kind: ConfigMap
metadata:
  name: log-service-config
  namespace: vpa-per-container-demo
data:
  log-service.py: |
    #!/usr/bin/env python3
    from flask import Flask, jsonify, request
    import os
    import time
    import random
    import threading
    import re
    
    app = Flask(__name__)
    
    # Log storage
    logs = []
    log_lock = threading.Lock()
    
    # Log levels and components for random log generation
    LOG_LEVELS = ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']
    COMPONENTS = ['api', 'database', 'auth', 'cache', 'network', 'ui']
    
    @app.route('/')
    def home():
        return jsonify({"service": "logger", "status": "running"})
    
    @app.route('/generate')
    def generate_logs():
        """Generate a specified number of random logs"""
        count = int(request.args.get('count', 100))
        
        new_logs = []
        timestamp = time.time()
        
        for i in range(count):
            level = random.choice(LOG_LEVELS)
            component = random.choice(COMPONENTS)
            message = f"Sample log message from {component} component with sequence {i}"
            
            log_entry = {
                "timestamp": timestamp + i/1000,
                "level": level,
                "component": component,
                "message": message
            }
            new_logs.append(log_entry)
        
        with log_lock:
            logs.extend(new_logs)
            # Keep only the most recent 10000 logs
            if len(logs) > 10000:
                logs = logs[-10000:]
        
        return jsonify({
            "status": "success",
            "generated": count,
            "total_logs": len(logs)
        })
    
    @app.route('/analyze')
    def analyze_logs():
        """Perform CPU-intensive analysis on logs"""
        with log_lock:
            log_count = len(logs)
            if log_count == 0:
                return jsonify({"status": "error", "message": "No logs to analyze"})
            
            local_logs = logs.copy()
        
        # Simulate CPU-intensive log analysis
        start_time = time.time()
        
        # Count logs by level
        level_counts = {}
        component_counts = {}
        error_patterns = {}
        
        # This loop is intentionally inefficient to generate CPU load
        for _ in range(3):  # Multiple passes to increase CPU usage
            for log in local_logs:
                # Count by level
                level = log['level']
                level_counts[level] = level_counts.get(level, 0) + 1
                
                # Count by component
                component = log['component']
                component_counts[component] = component_counts.get(component, 0) + 1
                
                # Look for error patterns in messages
                if level in ['ERROR', 'CRITICAL']:
                    message = log['message']
                    # Simulated pattern matching with regex
                    for pattern in ['database', 'timeout', 'connection', 'failed', 'exception']:
                        if re.search(pattern, message, re.IGNORECASE):
                            error_patterns[pattern] = error_patterns.get(pattern, 0) + 1
        
        processing_time = time.time() - start_time
        
        return jsonify({
            "status": "success",
            "analyzed_logs": log_count,
            "processing_time_seconds": processing_time,
            "level_distribution": level_counts,
            "component_distribution": component_counts,
            "error_patterns": error_patterns
        })
    
    @app.route('/status')
    def log_status():
        """Return log status"""
        with log_lock:
            log_count = len(logs)
            
            level_counts = {}
            for log in logs:
                level = log['level']
                level_counts[level] = level_counts.get(level, 0) + 1
        
        return jsonify({
            "total_logs": log_count,
            "level_counts": level_counts
        })
    
    if __name__ == '__main__':
        app.run(host='0.0.0.0', port=8082)
---
# Deployment with multiple containers
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-container-app
  namespace: vpa-per-container-demo
spec:
  selector:
    matchLabels:
      app: multi-container-app
  replicas: 2
  template:
    metadata:
      labels:
        app: multi-container-app
    spec:
      containers:
      # Main application container - web server
      - name: main-app
        image: nginx:1.21-alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 30Mi
          limits:
            cpu: 200m
            memory: 50Mi
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/conf.d/default.conf
          subPath: nginx.conf
        - name: web-content
          mountPath: /usr/share/nginx/html/index.html
          subPath: index.html
      
      # Cache container - higher memory usage
      - name: cache-service
        image: python:3.9-alpine
        command: ["python3", "/app/cache-service.py"]
        ports:
        - containerPort: 8081
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: cache-config
          mountPath: /app
      
      # Logger container - higher CPU usage
      - name: log-service
        image: python:3.9-alpine
        command: ["python3", "/app/log-service.py"]
        ports:
        - containerPort: 8082
        resources:
          requests:
            cpu: 100m
            memory: 50Mi
          limits:
            cpu: 200m
            memory: 100Mi
        volumeMounts:
        - name: log-config
          mountPath: /app
      
      volumes:
      - name: nginx-config
        configMap:
          name: main-app-config
          items:
          - key: nginx.conf
            path: nginx.conf
      - name: web-content
        configMap:
          name: main-app-config
          items:
          - key: index.html
            path: index.html
      - name: cache-config
        configMap:
          name: cache-service-config
      - name: log-config
        configMap:
          name: log-service-config
---
# ConfigMap for python dependencies
apiVersion: v1
kind: ConfigMap
metadata:
  name: python-requirements
  namespace: vpa-per-container-demo
data:
  requirements.txt: |
    flask==2.0.1
    werkzeug==2.0.1
---
# Init container job to install dependencies
apiVersion: batch/v1
kind: Job
metadata:
  name: install-dependencies
  namespace: vpa-per-container-demo
spec:
  template:
    spec:
      containers:
      - name: installer
        image: python:3.9-alpine
        command: ["/bin/sh", "-c"]
        args:
        - |
          echo "Installing dependencies..."
          pip install -r /requirements/requirements.txt
          echo "Dependencies installed:"
          pip freeze
          echo "Done!"
        volumeMounts:
        - name: requirements
          mountPath: /requirements
      volumes:
      - name: requirements
        configMap:
          name: python-requirements
      restartPolicy: Never
  backoffLimit: 4
---
# Service to expose the application
apiVersion: v1
kind: Service
metadata:
  name: multi-container-app
  namespace: vpa-per-container-demo
spec:
  selector:
    app: multi-container-app
  ports:
  - port: 80
    targetPort: 80
    name: http
---
# VPA with per-container configuration
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: multi-container-vpa
  namespace: vpa-per-container-demo
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: multi-container-app
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    # Main app container - moderate resource needs
    - containerName: main-app
      mode: "Auto"
      minAllowed:
        cpu: 50m
        memory: 20Mi
      maxAllowed:
        cpu: 300m
        memory: 100Mi
      controlledResources: ["cpu", "memory"]
    
    # Cache container - optimized for memory
    - containerName: cache-service
      mode: "Auto"
      minAllowed:
        cpu: 30m
        memory: 50Mi
      maxAllowed:
        cpu: 200m
        memory: 500Mi  # Higher memory limit for caching
      controlledValues: "RequestsAndLimits"
    
    # Logger container - optimized for CPU
    - containerName: log-service
      mode: "Auto"
      minAllowed:
        cpu: 50m
        memory: 30Mi
      maxAllowed:
        cpu: 500m  # Higher CPU limit for log processing
        memory: 150Mi
      controlledResources: ["cpu", "memory"]

# Test and Usage Instructions:
# 
# Prerequisites:
# 1. VPA components must be installed on your cluster
# 2. Install Python packages on the service containers
#
# Deployment:
# 1. Apply this manifest:
#    ```
#    kubectl apply -f 05-vpa-per-container.yaml
#    ```
#
# 2. Set up port forwarding to access the service:
#    ```
#    kubectl port-forward svc/multi-container-app 8080:80 -n vpa-per-container-demo
#    ```
#
# 3. Generate load for each container:
#    - Open http://localhost:8080 in your browser
#    - For the cache container: Click "Load Cache Data" to allocate memory
#    - For the logger container: Click "Generate Logs" and "Analyze Logs" to use CPU
#    
#    Or use curl:
#    ```
#    # Load cache with 100MB of data
#    curl "http://localhost:8080/api/cache/load?size_mb=100"
#    
#    # Generate 5000 logs
#    curl "http://localhost:8080/api/logs/generate?count=5000"
#    
#    # Analyze logs (CPU-intensive)
#    curl "http://localhost:8080/api/logs/analyze"
#    ```
#
# 4. Monitor VPA behavior for each container:
#    ```
#    # Check VPA recommendations for each container
#    kubectl describe vpa multi-container-vpa -n vpa-per-container-demo
#    
#    # Watch pods being restarted as VPA adjusts resources
#    kubectl get pods -n vpa-per-container-demo -w
#    
#    # Get detailed resource usage for each container
#    kubectl top pod -n vpa-per-container-demo --containers
#    ```
#
# Key points about per-container VPA:
# 1. Each container has different resource characteristics:
#    - Main app: Balanced needs
#    - Cache service: Memory-intensive 
#    - Log service: CPU-intensive
#
# 2. The VPA policies reflect these differences:
#    - Cache container has higher memory allowance
#    - Logger container has higher CPU allowance
#
# 3. This approach allows more efficient resource allocation in multi-container pods
#    where containers have very different resource profiles
#
# Notes:
# - In real applications, containers often have specialized roles with different resource needs
# - Per-container VPA configuration is particularly valuable in microservices or sidecar patterns
# - The example uses Python for simplicity, but the same principles apply to any containerized app 