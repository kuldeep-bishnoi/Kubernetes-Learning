# Custom Metrics Horizontal Pod Autoscaler Example
# This example demonstrates an HPA that scales based on custom/external metrics

---
# Note: This example requires the following prerequisites:
# 1. Prometheus installed in your cluster
# 2. Prometheus Adapter configured to expose custom metrics
# 3. ServiceMonitor CRD (for Prometheus Operator) or appropriate scrape config

---
# Create a namespace for our resources
apiVersion: v1
kind: Namespace
metadata:
  name: custom-metrics-demo
  labels:
    monitoring: enabled  # Used by Prometheus Operator to auto-discover ServiceMonitors

---
# ConfigMap for our example API server
apiVersion: v1
kind: ConfigMap
metadata:
  name: api-server-config
  namespace: custom-metrics-demo
data:
  app.py: |
    #!/usr/bin/env python3
    import time
    import random
    import os
    from flask import Flask, request, jsonify
    from prometheus_client import Counter, Histogram, generate_latest, REGISTRY
    import prometheus_client

    app = Flask(__name__)

    # Expose metrics on a separate port
    prometheus_client.start_http_server(9090)

    # Define Prometheus metrics
    REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP Requests', ['method', 'endpoint', 'status_code'])
    REQUEST_LATENCY = Histogram('http_request_duration_seconds', 'HTTP Request Latency', ['method', 'endpoint'])
    QUEUE_SIZE = prometheus_client.Gauge('app_queue_size', 'Current queue size')

    # Simulated queue
    queue = []

    @app.route('/')
    def home():
        REQUEST_COUNT.labels('GET', '/', '200').inc()
        return """
        API Server with Custom Metrics:
        - /metrics - Prometheus metrics endpoint
        - /enqueue?items=10 - Add items to the queue
        - /process?items=5 - Process items from the queue
        - /queue - Get current queue status
        """

    @app.route('/metrics')
    def metrics():
        return generate_latest(REGISTRY)

    @app.route('/enqueue')
    def enqueue():
        items = int(request.args.get('items', '1'))
        if items < 1:
            items = 1
        if items > 1000:
            items = 1000
            
        start = time.time()
        
        # Add items to the queue
        for i in range(items):
            queue.append(f"item-{time.time()}-{random.randint(1, 1000)}")
        
        # Update queue size metric
        QUEUE_SIZE.set(len(queue))
        
        latency = time.time() - start
        REQUEST_COUNT.labels('GET', '/enqueue', '200').inc()
        REQUEST_LATENCY.labels('GET', '/enqueue').observe(latency)
        
        return jsonify({
            "action": "enqueue",
            "items_added": items,
            "current_queue_size": len(queue)
        })

    @app.route('/process')
    def process():
        items = int(request.args.get('items', '1'))
        if items < 1:
            items = 1
            
        start = time.time()
        
        # Process items from the queue
        processed = 0
        while processed < items and len(queue) > 0:
            item = queue.pop(0)
            # Simulate processing time
            time.sleep(0.01)
            processed += 1
        
        # Update queue size metric
        QUEUE_SIZE.set(len(queue))
        
        latency = time.time() - start
        REQUEST_COUNT.labels('GET', '/process', '200').inc()
        REQUEST_LATENCY.labels('GET', '/process').observe(latency)
        
        return jsonify({
            "action": "process",
            "items_processed": processed,
            "current_queue_size": len(queue)
        })

    @app.route('/queue')
    def queue_status():
        REQUEST_COUNT.labels('GET', '/queue', '200').inc()
        return jsonify({
            "queue_size": len(queue),
            "queue_items": len(queue)
        })

    if __name__ == '__main__':
        # Set initial queue size metric
        QUEUE_SIZE.set(0)
        app.run(host='0.0.0.0', port=8080)

---
# Deployment for our API server with Prometheus instrumentation
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
  namespace: custom-metrics-demo
spec:
  selector:
    matchLabels:
      app: api-server
  replicas: 1
  template:
    metadata:
      labels:
        app: api-server
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: api-server
        image: python:3.9-slim
        command: ["/bin/sh", "-c"]
        args:
          - |
            pip install flask prometheus_client
            mkdir -p /app
            cp /scripts/app.py /app/
            cd /app
            python app.py
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: metrics
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "200m"
            memory: "256Mi"
        volumeMounts:
        - name: scripts
          mountPath: /scripts
      volumes:
      - name: scripts
        configMap:
          name: api-server-config
          defaultMode: 0777

---
# Service for the API server
apiVersion: v1
kind: Service
metadata:
  name: api-server
  namespace: custom-metrics-demo
  labels:
    app: api-server
spec:
  ports:
  - port: 8080
    name: http
    targetPort: 8080
  - port: 9090
    name: metrics
    targetPort: 9090
  selector:
    app: api-server
  type: ClusterIP

---
# ServiceMonitor for Prometheus Operator
# This is a Custom Resource that tells Prometheus Operator to scrape our service
# If not using Prometheus Operator, you'll need to configure Prometheus scrape config manually
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: api-server
  namespace: custom-metrics-demo
  labels:
    release: prometheus  # Match the label selector for your Prometheus instance
spec:
  selector:
    matchLabels:
      app: api-server
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics

---
# HorizontalPodAutoscaler based on a custom application metric (queue size)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-server-queue-hpa
  namespace: custom-metrics-demo
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  minReplicas: 1
  maxReplicas: 10
  metrics:
  # Custom metric from Prometheus
  - type: Pods
    pods:
      metric:
        name: app_queue_size  # The name of our custom metric
      target:
        type: AverageValue
        averageValue: 10      # Scale when average queue size exceeds 10 items per pod
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Pods
        value: 1                      # Add one pod at a time
        periodSeconds: 30
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60

---
# External metrics example (for reference)
# This would scale based on a metric from an external system (e.g., cloud provider)
# Note: This requires an external metrics adapter to be configured
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-server-external-hpa
  namespace: custom-metrics-demo
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  minReplicas: 1
  maxReplicas: 5
  metrics:
  # External metric example (AWS SQS queue length)
  - type: External
    external:
      metric:
        name: sqs_messages_visible
        selector:
          matchLabels:
            queue-name: my-queue
            region: us-west-2
      target:
        type: AverageValue
        averageValue: 30     # Scale when there are 30+ messages in the queue

---
# Instructions for testing:
# 
# Prerequisites:
# 1. Install Prometheus Operator:
#    kubectl apply -f https://github.com/prometheus-operator/prometheus-operator/releases/download/v0.59.1/bundle.yaml
#
# 2. Install Prometheus Adapter for custom metrics:
#    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
#    helm install prometheus-adapter prometheus-community/prometheus-adapter -n monitoring \
#      --set prometheus.url=http://prometheus-operated.monitoring.svc \
#      --set prometheus.port=9090
#
# 3. Configure Prometheus Adapter rules for your custom metrics:
#    This requires creating a ConfigMap with custom rules that map Prometheus metrics to HPA metrics.
#    See: https://github.com/kubernetes-sigs/prometheus-adapter/blob/master/docs/config.md
#
# Testing the HPA:
#
# 1. Apply this manifest:
#    kubectl apply -f 04-custom-metrics-hpa.yaml
#
# 2. Forward the service port to access the application:
#    kubectl -n custom-metrics-demo port-forward svc/api-server 8080:8080
#
# 3. Add items to the queue to trigger scaling:
#    curl "http://localhost:8080/enqueue?items=50"
#
# 4. Check the current queue size:
#    curl "http://localhost:8080/queue"
#
# 5. Check if the custom metrics are available to Kubernetes:
#    kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/custom-metrics-demo/pods/*/app_queue_size"
#
# 6. Watch the HPA status:
#    kubectl -n custom-metrics-demo get hpa api-server-queue-hpa -w
#
# 7. Process some items to reduce the queue size:
#    curl "http://localhost:8080/process?items=20"
#
# 8. Observe scaling behavior:
#    kubectl -n custom-metrics-demo get deployment api-server 